{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Simple_GAN.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"5ikuH6SyzZk2","colab_type":"text"},"source":["## Generative Adversarial Network\n","\n","Trains a GAN\n","\n","## How to use\n","1) Set the parameters to fit your needs and environment\n","\n","2) Run All blocks\n","\n","3) Allow google drive to be mounted\n","\n","4) Wait"]},{"cell_type":"markdown","metadata":{"id":"6Y160GSIJBoc","colab_type":"text"},"source":["### Parameters"]},{"cell_type":"code","metadata":{"id":"bxVTZKsoJE2z","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596313262044,"user_tz":420,"elapsed":763,"user":{"displayName":"Colton Davenport","photoUrl":"","userId":"13788767686338694290"}}},"source":["# Set true or false depending on wether training a new network \n","# or continuing to train an old one\n","#   If true, will start at the latest checkpoint in CHECKPOINT_DIR\n","CONTINUING_RUN = False\n","\n","# Name the run\n","#   This name is used for checkpoints and saving\n","#   network weights at the end of run \n","RUN_NAME = \"GAN_TEST\"\n","\n","# Directory path for the training and validation tfRecords\n","#   Training records are assumed to follow pattern train_*\n","#   Testing records are assumed to follow pattern val_*\n","RECORD_DIR = 'drive/My Drive/github_prep/welsh-200'\n","\n","# Name of folder to save checkpoints and model weights to\n","CHECKPOINT_DIR = 'drive/My Drive/github_prep/checkpoints'\n","\n","# Set the batch size, epochs, and steps per epoch for this run\n","EPOCHS = 250\n","STEPS_PER_EPOCH = 200\n","BATCH_SIZE = 768"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sdtQY6eYM1GS","colab_type":"text"},"source":["##Setup"]},{"cell_type":"code","metadata":{"id":"XiARWW1wFaK6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":128},"executionInfo":{"status":"ok","timestamp":1596313285071,"user_tz":420,"elapsed":23774,"user":{"displayName":"Colton Davenport","photoUrl":"","userId":"13788767686338694290"}},"outputId":"235a6ed4-e1e3-4d5d-bbf3-d24bc1abd187"},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"I1DQb4z3exnE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1596313504300,"user_tz":420,"elapsed":81995,"user":{"displayName":"Colton Davenport","photoUrl":"","userId":"13788767686338694290"}},"outputId":"d463b4e3-58f8-4e4a-e1a9-90d8bbcd3dd5"},"source":["!pip uninstall tensorflow --y\n","!pip3 install --upgrade tensorflow-gpu\n","!pip install grpcio>=1.24.3 --upgrade\n","!pip install tensorflow-addons\n","!pip install keras-rectified-adam"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Uninstalling tensorflow-2.2.0:\n","  Successfully uninstalled tensorflow-2.2.0\n","Collecting tensorflow-gpu\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/11/763f55d3d15efd778ef24453f126e6c33635680e5a2bb346da3fab5997cb/tensorflow_gpu-2.3.0-cp36-cp36m-manylinux2010_x86_64.whl (320.4MB)\n","\u001b[K     |████████████████████████████████| 320.4MB 50kB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.30.0)\n","Requirement already satisfied, skipping upgrade: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.15.0)\n","Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.9.0)\n","Requirement already satisfied, skipping upgrade: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.18.5)\n","Requirement already satisfied, skipping upgrade: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.12.2)\n","Requirement already satisfied, skipping upgrade: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.10.0)\n","Requirement already satisfied, skipping upgrade: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.4.1)\n","Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n","Requirement already satisfied, skipping upgrade: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.2.0)\n","Requirement already satisfied, skipping upgrade: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.6.3)\n","Requirement already satisfied, skipping upgrade: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.2)\n","Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.3.0)\n","Collecting tensorflow-estimator<2.4.0,>=2.3.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/ed/5853ec0ae380cba4588eab1524e18ece1583b65f7ae0e97321f5ff9dfd60/tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459kB)\n","\u001b[K     |████████████████████████████████| 460kB 7.8MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.34.2)\n","Requirement already satisfied, skipping upgrade: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.3.3)\n","Collecting tensorboard<3,>=2.3.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/1b/6a420d7e6ba431cf3d51b2a5bfa06a958c4141e3189385963dc7f6fbffb6/tensorboard-2.3.0-py3-none-any.whl (6.8MB)\n","\u001b[K     |████████████████████████████████| 6.8MB 51.7MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12.1)\n","Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tensorflow-gpu) (49.2.0)\n","Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (3.2.2)\n","Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (0.4.1)\n","Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (1.17.2)\n","Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (2.23.0)\n","Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (1.0.1)\n","Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (1.7.0)\n","Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow-gpu) (1.7.0)\n","Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow-gpu) (1.3.0)\n","Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu) (4.6)\n","Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu) (4.1.1)\n","Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu) (0.2.8)\n","Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu) (2020.6.20)\n","Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu) (3.0.4)\n","Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu) (2.10)\n","Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu) (1.24.3)\n","Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow-gpu) (3.1.0)\n","Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow-gpu) (3.1.0)\n","Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu) (0.4.8)\n","Installing collected packages: tensorflow-estimator, tensorboard, tensorflow-gpu\n","  Found existing installation: tensorflow-estimator 2.2.0\n","    Uninstalling tensorflow-estimator-2.2.0:\n","      Successfully uninstalled tensorflow-estimator-2.2.0\n","  Found existing installation: tensorboard 2.2.2\n","    Uninstalling tensorboard-2.2.2:\n","      Successfully uninstalled tensorboard-2.2.2\n","Successfully installed tensorboard-2.3.0 tensorflow-estimator-2.3.0 tensorflow-gpu-2.3.0\n","Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.6/dist-packages (0.8.3)\n","Requirement already satisfied: typeguard in /usr/local/lib/python3.6/dist-packages (from tensorflow-addons) (2.7.1)\n","Collecting keras-rectified-adam\n","  Downloading https://files.pythonhosted.org/packages/21/79/9521f66b92186702cb58a214c1b923b416266381cd824e15a1733f6a5b06/keras-rectified-adam-0.17.0.tar.gz\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-rectified-adam) (1.18.5)\n","Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras-rectified-adam) (2.3.1)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras-rectified-adam) (2.10.0)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-rectified-adam) (1.4.1)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras-rectified-adam) (3.13)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-rectified-adam) (1.1.2)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-rectified-adam) (1.0.8)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-rectified-adam) (1.15.0)\n","Building wheels for collected packages: keras-rectified-adam\n","  Building wheel for keras-rectified-adam (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-rectified-adam: filename=keras_rectified_adam-0.17.0-cp36-none-any.whl size=14781 sha256=ca315f2a312db6603af10187464a8b6fbc5f2622340f26d2cc14942828a03572\n","  Stored in directory: /root/.cache/pip/wheels/7b/01/27/3a934e1a5644f5b93c720422a6ef97034ea78a21ba71cfb549\n","Successfully built keras-rectified-adam\n","Installing collected packages: keras-rectified-adam\n","Successfully installed keras-rectified-adam-0.17.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mewZcGlcJHg6","colab_type":"text"},"source":["### Imports"]},{"cell_type":"code","metadata":{"id":"LlhYuuWAJJQC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1596313508378,"user_tz":420,"elapsed":71158,"user":{"displayName":"Colton Davenport","photoUrl":"","userId":"13788767686338694290"}},"outputId":"6b983ba9-fd73-45ee-f4ee-5dd96fbbcb9c"},"source":["import tensorflow as tf\n","\n","import os\n","import time\n","\n","from keras_radam import RAdam\n","\n","import librosa\n","import pandas as pd\n","import os\n","import datetime\n","import numpy as np\n","import tensorflow as tf\n","import glob\n","from sklearn.utils import shuffle\n","\n","\n","from tensorflow.keras.layers import Input, Reshape, Dropout, Dense \n","from tensorflow.keras.layers import Flatten, BatchNormalization\n","from tensorflow.keras.layers import Activation, ZeroPadding2D\n","from tensorflow.keras.layers import LeakyReLU\n","from tensorflow.keras.layers import UpSampling2D, Conv2D, Conv2DTranspose\n","from tensorflow.keras.models import Sequential, Model, load_model\n","from tensorflow.keras.optimizers import Adam\n","\n","\n","import time"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"9xG5rV3pJfL9","colab_type":"text"},"source":["### Create Dataset"]},{"cell_type":"code","metadata":{"id":"pcdIKeNJJNiK","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596313508381,"user_tz":420,"elapsed":71152,"user":{"displayName":"Colton Davenport","photoUrl":"","userId":"13788767686338694290"}}},"source":["def tf_record_parser(record):\n","    keys_to_features = {\n","        \"noise_stft_phase\": tf.io.FixedLenFeature((), tf.string, default_value=\"\"),\n","        'noise_stft_mag_features': tf.io.FixedLenFeature([], tf.string),\n","        \"clean_stft_magnitude\": tf.io.FixedLenFeature((), tf.string)\n","    }\n","\n","    features = tf.io.parse_single_example(record, keys_to_features)\n","\n","    noise_stft_mag_features = tf.io.decode_raw(features['noise_stft_mag_features'], tf.float32)\n","    clean_stft_magnitude = tf.io.decode_raw(features['clean_stft_magnitude'], tf.float32)\n","    noise_stft_phase = tf.io.decode_raw(features['noise_stft_phase'], tf.float32)\n","\n","    # reshape input and annotation images\n","    noise_stft_mag_features = tf.reshape(noise_stft_mag_features, (129, 8, 1), name=\"noise_stft_mag_features\")\n","    clean_stft_magnitude = tf.reshape(clean_stft_magnitude, (129, 1, 1), name=\"clean_stft_magnitude\")\n","    noise_stft_phase = tf.reshape(noise_stft_phase, (129,), name=\"noise_stft_phase\")\n","\n","    return noise_stft_mag_features, clean_stft_magnitude"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"M-8ZH-3iJZSb","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596313508607,"user_tz":420,"elapsed":71372,"user":{"displayName":"Colton Davenport","photoUrl":"","userId":"13788767686338694290"}}},"source":["\n","train_tfrecords_filenames = glob.glob(RECORD_DIR + '/train_*')\n","np.random.shuffle(train_tfrecords_filenames)\n","train_tfrecords_filenames = list(train_tfrecords_filenames)\n","val_tfrecords_filenames = glob.glob(RECORD_DIR + '/val_*')\n","\n","#build the training dataset\n","train_dataset = tf.data.TFRecordDataset([train_tfrecords_filenames])\n","train_dataset = train_dataset.map(tf_record_parser)\n","train_dataset = train_dataset.shuffle(8192)\n","train_dataset = train_dataset.repeat()\n","train_dataset = train_dataset.batch(BATCH_SIZE)\n","train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n","\n"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"UD4bRTRdP1X4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1596313508610,"user_tz":420,"elapsed":71370,"user":{"displayName":"Colton Davenport","photoUrl":"","userId":"13788767686338694290"}},"outputId":"2a70dccf-be58-4572-b9a9-1382544e901a"},"source":["print(train_tfrecords_filenames)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["['drive/My Drive/github_prep/welsh-200/train_0.tfrecords', 'drive/My Drive/github_prep/welsh-200/train_0 (1).tfrecords']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3pATZ76nJjza","colab_type":"text"},"source":["## Define GAN"]},{"cell_type":"code","metadata":{"id":"HCedNEBdJmXS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":144},"executionInfo":{"status":"ok","timestamp":1596313508611,"user_tz":420,"elapsed":71361,"user":{"displayName":"Colton Davenport","photoUrl":"","userId":"13788767686338694290"}},"outputId":"c12b59f1-75c1-490c-abf9-40d69c11801e"},"source":["OUTPUT_CHANNELS = 3\n","\n","windowLength = 256\n","overlap      = round(0.25 * windowLength) # overlap of 75%\n","ffTLength    = windowLength\n","inputFs      = 48e3\n","fs           = 16e3\n","numFeatures  = ffTLength//2 + 1\n","numSegments  = 8\n","print(\"windowLength:\",windowLength)\n","print(\"overlap:\",overlap)\n","print(\"ffTLength:\",ffTLength)\n","print(\"inputFs:\",inputFs)\n","print(\"fs:\",fs)\n","print(\"numFeatures:\",numFeatures)\n","print(\"numSegments:\",numSegments)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["windowLength: 256\n","overlap: 64\n","ffTLength: 256\n","inputFs: 48000.0\n","fs: 16000.0\n","numFeatures: 129\n","numSegments: 8\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LO3-Jx4-JmrU","colab_type":"text"},"source":["### Define Generator"]},{"cell_type":"code","metadata":{"id":"7nZ091aTJjNC","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596313509185,"user_tz":420,"elapsed":71930,"user":{"displayName":"Colton Davenport","photoUrl":"","userId":"13788767686338694290"}}},"source":["def conv_block(x, filters, kernel_size, strides, padding='same', use_bn=True):\n","  x = Conv2D(filters=filters, kernel_size=kernel_size, strides=strides, padding=padding, use_bias=False,\n","              kernel_regularizer=tf.keras.regularizers.l2(0.0006))(x)\n","  x = Activation('relu')(x)\n","  if use_bn:\n","    x = BatchNormalization()(x)\n","  return x\n","\n","def full_pre_activation_block(x, filters, kernel_size, strides, padding='same', use_bn=True):\n","  shortcut = x\n","  in_channels = x.shape[-1]\n","\n","  x = BatchNormalization()(x)\n","  x = Activation('relu')(x)\n","  x = Conv2D(filters=filters, kernel_size=kernel_size, strides=strides, padding='same')(x)\n","\n","  x = BatchNormalization()(x)\n","  x = Activation('relu')(x)\n","  x = Conv2D(filters=in_channels, kernel_size=kernel_size, strides=strides, padding='same')(x)\n","\n","  return shortcut + x\n","\n","def Generator(l2_strength=0.0):\n","  inputs = Input(shape=[numFeatures,numSegments,1])\n","  x = inputs\n","\n","  # -----\n","  x = tf.keras.layers.ZeroPadding2D(((4,4), (0,0)))(x)\n","  x = Conv2D(filters=18, kernel_size=[9,8], strides=[1, 1], padding='valid', use_bias=False,\n","              kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n","  x = Activation('relu')(x)\n","  x = BatchNormalization()(x)\n","\n","  skip0 = Conv2D(filters=30, kernel_size=[5,1], strides=[1, 1], padding='same', use_bias=False,\n","                 kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n","  x = Activation('relu')(skip0)\n","  x = BatchNormalization()(x)\n","\n","  x = Conv2D(filters=8, kernel_size=[9,1], strides=[1, 1], padding='same', use_bias=False,\n","              kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n","  x = Activation('relu')(x)\n","  x = BatchNormalization()(x)\n","\n","  # -----\n","  x = Conv2D(filters=18, kernel_size=[9,1], strides=[1, 1], padding='same', use_bias=False,\n","              kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n","  x = Activation('relu')(x)\n","  x = BatchNormalization()(x)\n","\n","  skip1 = Conv2D(filters=30, kernel_size=[5,1], strides=[1, 1], padding='same', use_bias=False,\n","                 kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n","  x = Activation('relu')(skip1)\n","  x = BatchNormalization()(x)\n","\n","  x = Conv2D(filters=8, kernel_size=[9,1], strides=[1, 1], padding='same', use_bias=False,\n","              kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n","  x = Activation('relu')(x)\n","  x = BatchNormalization()(x)\n","\n","  # ----\n","  x = Conv2D(filters=18, kernel_size=[9,1], strides=[1, 1], padding='same', use_bias=False,\n","              kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n","  x = Activation('relu')(x)\n","  x = BatchNormalization()(x)\n","  \n","  x = Conv2D(filters=30, kernel_size=[5,1], strides=[1, 1], padding='same', use_bias=False,\n","              kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n","  x = Activation('relu')(x)\n","  x = BatchNormalization()(x)\n","\n","  x = Conv2D(filters=8, kernel_size=[9,1], strides=[1, 1], padding='same', use_bias=False,\n","              kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n","  x = Activation('relu')(x)\n","  x = BatchNormalization()(x)\n","\n","  # ----\n","  x = Conv2D(filters=18, kernel_size=[9,1], strides=[1, 1], padding='same', use_bias=False,\n","              kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n","  x = Activation('relu')(x)\n","  x = BatchNormalization()(x)\n","\n","  x = Conv2D(filters=30, kernel_size=[5,1], strides=[1, 1], padding='same', use_bias=False,\n","             kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n","  x = x + skip1\n","  x = Activation('relu')(x)\n","  x = BatchNormalization()(x)\n","\n","  x = Conv2D(filters=8, kernel_size=[9,1], strides=[1, 1], padding='same', use_bias=False,\n","              kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n","  x = Activation('relu')(x)\n","  x = BatchNormalization()(x)\n","\n","  # ----\n","  x = Conv2D(filters=18, kernel_size=[9,1], strides=[1, 1], padding='same', use_bias=False,\n","              kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n","  x = Activation('relu')(x)\n","  x = BatchNormalization()(x)\n","\n","  x = Conv2D(filters=30, kernel_size=[5,1], strides=[1, 1], padding='same', use_bias=False,\n","             kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n","  x = x + skip0\n","  x = Activation('relu')(x)\n","  x = BatchNormalization()(x)\n","\n","  x = Conv2D(filters=8, kernel_size=[9,1], strides=[1, 1], padding='same', use_bias=False,\n","              kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n","  x = Activation('relu')(x)\n","  x = BatchNormalization()(x)\n","\n","  # ----\n","  x = tf.keras.layers.SpatialDropout2D(0.2)(x)\n","  x = Conv2D(filters=1, kernel_size=[129,1], strides=[1, 1], padding='same')(x)\n","\n","  model = Model(inputs=inputs, outputs=x)\n","\n","  optimizer = tf.keras.optimizers.Adam(3e-4)\n","  #optimizer = RAdam(total_steps=10000, warmup_proportion=0.1, min_lr=3e-4)\n","\n","  model.compile(optimizer=optimizer, loss='mse', \n","                metrics=[tf.keras.metrics.RootMeanSquaredError('rmse')])\n","  return model"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CZytl9AKJ8lx","colab_type":"text"},"source":["#### Generator Loss"]},{"cell_type":"code","metadata":{"id":"OpyIMxKiK1la","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596313509188,"user_tz":420,"elapsed":71927,"user":{"displayName":"Colton Davenport","photoUrl":"","userId":"13788767686338694290"}}},"source":["LAMBDA = 100\n","\n","def generator_loss(disc_generated_output, gen_output, target):\n","  gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n","\n","  # mean absolute error\n","  l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n","\n","  total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n","\n","  return total_gen_loss, gan_loss, l1_loss"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VlgjagNZK4za","colab_type":"text"},"source":["### Define Discriminator"]},{"cell_type":"code","metadata":{"id":"cJ_08OQVK9Lp","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596313509192,"user_tz":420,"elapsed":71926,"user":{"displayName":"Colton Davenport","photoUrl":"","userId":"13788767686338694290"}}},"source":["def downsample(filters, size, apply_batchnorm=True):\n","  initializer = tf.random_normal_initializer(0., 0.02)\n","\n","  result = tf.keras.Sequential()\n","  result.add(\n","      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n","                             kernel_initializer=initializer, use_bias=False))\n","\n","  if apply_batchnorm:\n","    result.add(tf.keras.layers.BatchNormalization())\n","\n","  result.add(tf.keras.layers.LeakyReLU())\n","\n","  return result\n","\n","def upsample(filters, size, apply_dropout=False):\n","  initializer = tf.random_normal_initializer(0., 0.02)\n","\n","  result = tf.keras.Sequential()\n","  result.add(\n","    tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n","                                    padding='same',\n","                                    kernel_initializer=initializer,\n","                                    use_bias=False))\n","\n","  result.add(tf.keras.layers.BatchNormalization())\n","\n","  if apply_dropout:\n","      result.add(tf.keras.layers.Dropout(0.5))\n","\n","  result.add(tf.keras.layers.ReLU())\n","\n","  return result"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"DdxMNB_1LBSZ","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596313509193,"user_tz":420,"elapsed":71922,"user":{"displayName":"Colton Davenport","photoUrl":"","userId":"13788767686338694290"}}},"source":["def Discriminator():\n","  initializer = tf.random_normal_initializer(0., 0.02)\n","\n","  inp = tf.keras.layers.Input(shape=[129, 1, 1], name='input_image')\n","  tar = tf.keras.layers.Input(shape=[129, 1, 1], name='target_image')\n","\n","\n","  x = tf.keras.layers.concatenate([inp, tar]) # (bs, 256, 256, channels*2)\n","\n","  down1 = downsample(64, 4, False)(x) # (bs, 128, 128, 64)\n","  down2 = downsample(128, 4)(down1) # (bs, 64, 64, 128)\n","  down3 = downsample(256, 4)(down2) # (bs, 32, 32, 256)\n","\n","  zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3) # (bs, 34, 34, 256)\n","  \n","  conv = tf.keras.layers.Conv2D(512, (2,1), strides=1,\n","                                kernel_initializer=initializer,\n","                                use_bias=False)(zero_pad1) # (bs, 31, 31, 512)\n","\n","  batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n","\n","  leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n","\n","  zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu) # (bs, 33, 33, 512)\n","\n","  last = tf.keras.layers.Conv2D(1, 4, strides=1,\n","                                kernel_initializer=initializer)(zero_pad2) # (bs, 30, 30, 1)\n","\n","  return tf.keras.Model(inputs=[inp, tar], outputs=last)\n"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EzBYPWQ8LGWB","colab_type":"text"},"source":["#### Discriminator Loss"]},{"cell_type":"code","metadata":{"id":"CzMuma4rLI1J","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596313509194,"user_tz":420,"elapsed":71920,"user":{"displayName":"Colton Davenport","photoUrl":"","userId":"13788767686338694290"}}},"source":["loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n","\n","def discriminator_loss(disc_real_output, disc_generated_output):\n","  real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n","\n","  generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n","\n","  total_disc_loss = real_loss + generated_loss\n","\n","  return total_disc_loss"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6RqVDayPLQrq","colab_type":"text"},"source":["### Optimizers"]},{"cell_type":"code","metadata":{"id":"APQ1CvLVLTxR","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596313509195,"user_tz":420,"elapsed":71915,"user":{"displayName":"Colton Davenport","photoUrl":"","userId":"13788767686338694290"}}},"source":["generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n","discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NI-Kq_9uL0Fp","colab_type":"text"},"source":["## Training"]},{"cell_type":"code","metadata":{"id":"wU6SixaqPbiY","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596313509404,"user_tz":420,"elapsed":72120,"user":{"displayName":"Colton Davenport","photoUrl":"","userId":"13788767686338694290"}}},"source":["#Build the Networks\n","generator = Generator()\n","discriminator = Discriminator()"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WBSVsmv-Lhao","colab_type":"text"},"source":["###Checkpoint Saver"]},{"cell_type":"code","metadata":{"id":"29wioB60Llhp","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596313509405,"user_tz":420,"elapsed":72118,"user":{"displayName":"Colton Davenport","photoUrl":"","userId":"13788767686338694290"}}},"source":["checkpoint_prefix = os.path.join(CHECKPOINT_DIR, \"ckpt\" + \"_\" + RUN_NAME)\n","checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n","                                 discriminator_optimizer=discriminator_optimizer,\n","                                 generator=generator,\n","                                 discriminator=discriminator)"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3Zq3YM4MMDho","colab_type":"text"},"source":["### Define Training Methods"]},{"cell_type":"code","metadata":{"id":"Wf0xvQ_iIsEO","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596313509407,"user_tz":420,"elapsed":72115,"user":{"displayName":"Colton Davenport","photoUrl":"","userId":"13788767686338694290"}}},"source":["log_dir=\"logs/\"\n","\n","summary_writer = tf.summary.create_file_writer(\n","  log_dir + \"fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n","\n","@tf.function\n","def train_step(input_image, target, epoch):\n","  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n","    gen_output = generator(input_image, training=True)\n","\n","    disc_real_output = discriminator([input_image[:,:,7,:], target], training=True)\n","    disc_generated_output = discriminator([input_image[:,:,7,:], gen_output], training=True)\n","\n","    gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_generated_output, gen_output, target)\n","    disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n","\n","  generator_gradients = gen_tape.gradient(gen_total_loss,\n","                                          generator.trainable_variables)\n","  discriminator_gradients = disc_tape.gradient(disc_loss,\n","                                               discriminator.trainable_variables)\n","\n","  generator_optimizer.apply_gradients(zip(generator_gradients,\n","                                          generator.trainable_variables))\n","  discriminator_optimizer.apply_gradients(zip(discriminator_gradients,\n","                                              discriminator.trainable_variables))\n","\n","  with summary_writer.as_default():\n","    tf.summary.scalar('gen_total_loss', gen_total_loss, step=epoch)\n","    tf.summary.scalar('gen_gan_loss', gen_gan_loss, step=epoch)\n","    tf.summary.scalar('gen_l1_loss', gen_l1_loss, step=epoch)\n","    tf.summary.scalar('disc_loss', disc_loss, step=epoch)\n","\n","def fit(train_ds, epochs, steps_per_epoch):\n","  for epoch in range(epochs):\n","    start = time.time()\n","\n","    print(\"epoch start: \", epoch + 1)\n","    print(\" [\", end=\"\",flush=True)\n","    ten_percent_step = int(steps_per_epoch / 10)+1\n","    step_count = 0\n","    for (input_image, target) in train_ds:\n","      if step_count == steps_per_epoch:\n","        break\n","      step_count += 1\n","      if step_count % ten_percent_step == 0:\n","        print(\".\", end=\"\",flush=True)\n","      train_step(input_image, target, epoch)\n","    print(\".]\")\n","\n","    # saving (checkpoint) the model every 20 epochs\n","    if (epoch + 1) % 20 == 0:\n","      checkpoint.save(file_prefix = checkpoint_prefix)\n","      print(\" checkpoint saved\")\n","\n","    print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n","                                                        time.time()-start))\n","  \n","  checkpoint.save(file_prefix = checkpoint_prefix)"],"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V-B90EDeMHc6","colab_type":"text"},"source":["### Train"]},{"cell_type":"code","metadata":{"id":"hS9Of7_oMLLh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":380},"executionInfo":{"status":"ok","timestamp":1596314537390,"user_tz":420,"elapsed":1100093,"user":{"displayName":"Colton Davenport","photoUrl":"","userId":"13788767686338694290"}},"outputId":"3057b49e-0965-42a6-d673-ff81ae269813"},"source":["# restore the latest checkpoint if continuing a run\n","if CONTINUING_RUN:\n","  checkpoint.restore(tf.train.latest_checkpoint(CHECKPOINT_DIR))\n","\n","# Train the GAN\n","fit(train_dataset, EPOCHS, STEPS_PER_EPOCH)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["epoch start:  1\n"," [.........]\n","Time taken for epoch 1 is 207.73328185081482 sec\n","\n","epoch start:  2\n"," [.........]\n","Time taken for epoch 2 is 203.8938238620758 sec\n","\n","epoch start:  3\n"," [.........]\n","Time taken for epoch 3 is 204.73641419410706 sec\n","\n","epoch start:  4\n"," [.........]\n","Time taken for epoch 4 is 205.55988597869873 sec\n","\n","epoch start:  5\n"," [.........]\n","Time taken for epoch 5 is 205.04153895378113 sec\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Vlsjn0WKPZkx","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596314537401,"user_tz":420,"elapsed":1100101,"user":{"displayName":"Colton Davenport","photoUrl":"","userId":"13788767686338694290"}}},"source":["# Save the network weights\n","generator.save(CHECKPOINT_DIR + '/' + RUN_NAME + '_generator.h5')\n","discriminator.save(CHECKPOINT_DIR + '/' + RUN_NAME + '_discriminator.h5')"],"execution_count":20,"outputs":[]}]}