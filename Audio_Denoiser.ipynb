{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Audio_Denoiser.ipynb","provenance":[],"collapsed_sections":["CEG5JofLSfRw"]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"5ikuH6SyzZk2","colab_type":"text"},"source":["##Audio Denoiser\n","\n","Takes wav files and attempts to denoise them using a CNN \n","\n","## How to use\n","1) Set the environment parameters to appropriate folders and file in your drive\n","\n","2) Run All blocks\n","\n","3) Allow google drive to be mounted\n","\n","4) Wait"]},{"cell_type":"markdown","metadata":{"id":"wF1pz4-F6W_K","colab_type":"text"},"source":["###Environment Parameters"]},{"cell_type":"code","metadata":{"id":"X_x9tostyVHE","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596314781987,"user_tz":420,"elapsed":801,"user":{"displayName":"Colton Davenport","photoUrl":"","userId":"13788767686338694290"}}},"source":["# The complete path to a file containing the model weights\n","WEIGHTS_PATH = \"drive/My Drive/github_prep/checkpoints/GAN_TEST_generator.h5\"\n","\n","# The directory that will store the denoised audio clips\n","RESULTS_DIR = 'drive/My Drive/github_prep/denoised/GAN_TEST'\n","\n","# The source directory for noisy audio\n","#   noisy files are assumed to follow format noisy_#.wav\n","NOISY_DIR = 'drive/My Drive/github_prep/noisy_voices'"],"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OIiM-s9uxJUc","colab_type":"text"},"source":["##Setup"]},{"cell_type":"code","metadata":{"id":"XiARWW1wFaK6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1596314783094,"user_tz":420,"elapsed":1893,"user":{"displayName":"Colton Davenport","photoUrl":"","userId":"13788767686338694290"}},"outputId":"40173844-e2c4-40db-dbea-9a5271377f91"},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":21,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Ah7ISzvtEmur","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596314783096,"user_tz":420,"elapsed":1887,"user":{"displayName":"Colton Davenport","photoUrl":"","userId":"13788767686338694290"}}},"source":["import librosa\n","import os\n","import numpy as np\n","import tensorflow as tf\n","import scipy\n","import glob\n","import numpy as np"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"jKZtPoLMEmvJ","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596314783098,"user_tz":420,"elapsed":1882,"user":{"displayName":"Colton Davenport","photoUrl":"","userId":"13788767686338694290"}}},"source":["windowLength = 256\n","overlap      = round(0.25 * windowLength) # overlap of 75%\n","ffTLength    = windowLength\n","inputFs      = 48e3\n","fs           = 16e3\n","numFeatures  = ffTLength//2 + 1\n","numSegments  = 8"],"execution_count":23,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CEG5JofLSfRw","colab_type":"text"},"source":["## Model Definition"]},{"cell_type":"code","metadata":{"id":"2OZFegXrSYle","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596314783099,"user_tz":420,"elapsed":1877,"user":{"displayName":"Colton Davenport","photoUrl":"","userId":"13788767686338694290"}}},"source":["from tensorflow.keras.layers import Conv2D, Input, LeakyReLU, Flatten, Dense, Reshape, Conv2DTranspose, BatchNormalization, Activation\n","from tensorflow.keras import Model, Sequential"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"lc2K0cfhPU5-","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596314783100,"user_tz":420,"elapsed":1873,"user":{"displayName":"Colton Davenport","photoUrl":"","userId":"13788767686338694290"}}},"source":["def conv_block(x, filters, kernel_size, strides, padding='same', use_bn=True):\n","  x = Conv2D(filters=filters, kernel_size=kernel_size, strides=strides, padding=padding, use_bias=False,\n","              kernel_regularizer=tf.keras.regularizers.l2(0.0006))(x)\n","  x = Activation('relu')(x)\n","  if use_bn:\n","    x = BatchNormalization()(x)\n","  return x"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"XFYyKoAVQYCz","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596314783101,"user_tz":420,"elapsed":1867,"user":{"displayName":"Colton Davenport","photoUrl":"","userId":"13788767686338694290"}}},"source":["def full_pre_activation_block(x, filters, kernel_size, strides, padding='same', use_bn=True):\n","  shortcut = x\n","  in_channels = x.shape[-1]\n","\n","  x = BatchNormalization()(x)\n","  x = Activation('relu')(x)\n","  x = Conv2D(filters=filters, kernel_size=kernel_size, strides=strides, padding='same')(x)\n","\n","  x = BatchNormalization()(x)\n","  x = Activation('relu')(x)\n","  x = Conv2D(filters=in_channels, kernel_size=kernel_size, strides=strides, padding='same')(x)\n","\n","  return shortcut + x"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"NlRQ3ngpZG1Y","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596314783275,"user_tz":420,"elapsed":2035,"user":{"displayName":"Colton Davenport","photoUrl":"","userId":"13788767686338694290"}}},"source":["def build_model(l2_strength):\n","  inputs = Input(shape=[numFeatures,numSegments,1])\n","  x = inputs\n","\n","  # -----\n","  x = tf.keras.layers.ZeroPadding2D(((4,4), (0,0)))(x)\n","  x = Conv2D(filters=18, kernel_size=[9,8], strides=[1, 1], padding='valid', use_bias=False,\n","              kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n","  x = Activation('relu')(x)\n","  x = BatchNormalization()(x)\n","\n","  skip0 = Conv2D(filters=30, kernel_size=[5,1], strides=[1, 1], padding='same', use_bias=False,\n","                 kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n","  x = Activation('relu')(skip0)\n","  x = BatchNormalization()(x)\n","\n","  x = Conv2D(filters=8, kernel_size=[9,1], strides=[1, 1], padding='same', use_bias=False,\n","              kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n","  x = Activation('relu')(x)\n","  x = BatchNormalization()(x)\n","\n","  # -----\n","  x = Conv2D(filters=18, kernel_size=[9,1], strides=[1, 1], padding='same', use_bias=False,\n","              kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n","  x = Activation('relu')(x)\n","  x = BatchNormalization()(x)\n","\n","  skip1 = Conv2D(filters=30, kernel_size=[5,1], strides=[1, 1], padding='same', use_bias=False,\n","                 kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n","  x = Activation('relu')(skip1)\n","  x = BatchNormalization()(x)\n","\n","  x = Conv2D(filters=8, kernel_size=[9,1], strides=[1, 1], padding='same', use_bias=False,\n","              kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n","  x = Activation('relu')(x)\n","  x = BatchNormalization()(x)\n","\n","  # ----\n","  x = Conv2D(filters=18, kernel_size=[9,1], strides=[1, 1], padding='same', use_bias=False,\n","              kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n","  x = Activation('relu')(x)\n","  x = BatchNormalization()(x)\n","  \n","  x = Conv2D(filters=30, kernel_size=[5,1], strides=[1, 1], padding='same', use_bias=False,\n","              kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n","  x = Activation('relu')(x)\n","  x = BatchNormalization()(x)\n","\n","  x = Conv2D(filters=8, kernel_size=[9,1], strides=[1, 1], padding='same', use_bias=False,\n","              kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n","  x = Activation('relu')(x)\n","  x = BatchNormalization()(x)\n","\n","  # ----\n","  x = Conv2D(filters=18, kernel_size=[9,1], strides=[1, 1], padding='same', use_bias=False,\n","              kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n","  x = Activation('relu')(x)\n","  x = BatchNormalization()(x)\n","\n","  x = Conv2D(filters=30, kernel_size=[5,1], strides=[1, 1], padding='same', use_bias=False,\n","             kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n","  x = x + skip1\n","  x = Activation('relu')(x)\n","  x = BatchNormalization()(x)\n","\n","  x = Conv2D(filters=8, kernel_size=[9,1], strides=[1, 1], padding='same', use_bias=False,\n","              kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n","  x = Activation('relu')(x)\n","  x = BatchNormalization()(x)\n","\n","  # ----\n","  x = Conv2D(filters=18, kernel_size=[9,1], strides=[1, 1], padding='same', use_bias=False,\n","              kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n","  x = Activation('relu')(x)\n","  x = BatchNormalization()(x)\n","\n","  x = Conv2D(filters=30, kernel_size=[5,1], strides=[1, 1], padding='same', use_bias=False,\n","             kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n","  x = x + skip0\n","  x = Activation('relu')(x)\n","  x = BatchNormalization()(x)\n","\n","  x = Conv2D(filters=8, kernel_size=[9,1], strides=[1, 1], padding='same', use_bias=False,\n","              kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n","  x = Activation('relu')(x)\n","  x = BatchNormalization()(x)\n","\n","  # ----\n","  x = tf.keras.layers.SpatialDropout2D(0.2)(x)\n","  x = Conv2D(filters=1, kernel_size=[129,1], strides=[1, 1], padding='same')(x)\n","\n","  model = Model(inputs=inputs, outputs=x)\n","\n","  optimizer = tf.keras.optimizers.Adam(3e-4)\n","  #optimizer = RAdam(total_steps=10000, warmup_proportion=0.1, min_lr=3e-4)\n","\n","  model.compile(optimizer=optimizer, loss='mse', \n","                metrics=[tf.keras.metrics.RootMeanSquaredError('rmse')])\n","  return model"],"execution_count":27,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zegwr0_K2jse","colab_type":"text"},"source":["## Method Definitions"]},{"cell_type":"code","metadata":{"id":"DLiTiK8blE0n","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596314783276,"user_tz":420,"elapsed":2032,"user":{"displayName":"Colton Davenport","photoUrl":"","userId":"13788767686338694290"}}},"source":["def read_audio(filepath, sample_rate, normalize=True):\n","    # print(f\"Reading: {filepath}\").\n","    audio, sr = librosa.load(filepath, sr=sample_rate)\n","    if normalize:\n","      div_fac = 1 / np.max(np.abs(audio)) / 3.0\n","      audio = audio * div_fac\n","    return audio, sr"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"id":"uM6ajbBFlx3b","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596314783277,"user_tz":420,"elapsed":2028,"user":{"displayName":"Colton Davenport","photoUrl":"","userId":"13788767686338694290"}}},"source":["class FeatureExtractor:\n","    def __init__(self, audio, *, windowLength, overlap, sample_rate):\n","        self.audio = audio\n","        self.ffT_length = windowLength\n","        self.window_length = windowLength\n","        self.overlap = overlap\n","        self.sample_rate = sample_rate\n","        self.window = scipy.signal.hamming(self.window_length, sym=False)\n","\n","    def get_stft_spectrogram(self):\n","        return librosa.stft(self.audio, n_fft=self.ffT_length, win_length=self.window_length, hop_length=self.overlap,\n","                            window=self.window, center=True)\n","\n","    def get_audio_from_stft_spectrogram(self, stft_features):\n","        return librosa.istft(stft_features, win_length=self.window_length, hop_length=self.overlap,\n","                             window=self.window, center=True)\n","\n","    def get_mel_spectrogram(self):\n","        return librosa.feature.melspectrogram(self.audio, sr=self.sample_rate, power=2.0, pad_mode='reflect',\n","                                           n_fft=self.ffT_length, hop_length=self.overlap, center=True)\n","\n","    def get_audio_from_mel_spectrogram(self, M):\n","        return librosa.feature.inverse.mel_to_audio(M, sr=self.sample_rate, n_fft=self.ffT_length, hop_length=self.overlap,\n","                                             win_length=self.window_length, window=self.window,\n","                                             center=True, pad_mode='reflect', power=2.0, n_iter=32, length=None)"],"execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"id":"75M29dl3bBeF","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596314783278,"user_tz":420,"elapsed":2024,"user":{"displayName":"Colton Davenport","photoUrl":"","userId":"13788767686338694290"}}},"source":["def prepare_input_features(stft_features):\n","    # Phase Aware Scaling: To avoid extreme differences (more than\n","    # 45 degree) between the noisy and clean phase, the clean spectral magnitude was encoded as similar to [21]:\n","    noisySTFT = np.concatenate([stft_features[:,0:numSegments-1], stft_features], axis=1)\n","    stftSegments = np.zeros((numFeatures, numSegments , noisySTFT.shape[1] - numSegments + 1))\n","\n","    for index in range(noisySTFT.shape[1] - numSegments + 1):\n","        stftSegments[:,:,index] = noisySTFT[:,index:index + numSegments]\n","    return stftSegments"],"execution_count":30,"outputs":[]},{"cell_type":"code","metadata":{"id":"LzCga3PdUVwG","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596314783278,"user_tz":420,"elapsed":2019,"user":{"displayName":"Colton Davenport","photoUrl":"","userId":"13788767686338694290"}}},"source":["def revert_features_to_audio(noiseAudioFeatureExtractor, features, phase, cleanMean=None, cleanStd=None):\n","    # scale the outpus back to the original range\n","    if cleanMean and cleanStd:\n","        features = cleanStd * features + cleanMean\n","\n","    phase = np.transpose(phase, (1, 0))\n","    features = np.squeeze(features)\n","\n","    # features = librosa.db_to_power(features)\n","    features = features * np.exp(1j * phase)  # that fixes the abs() ope previously done\n","\n","    features = np.transpose(features, (1, 0))\n","    return noiseAudioFeatureExtractor.get_audio_from_stft_spectrogram(features)"],"execution_count":31,"outputs":[]},{"cell_type":"code","metadata":{"id":"phuR4eze2EVt","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596314783279,"user_tz":420,"elapsed":2015,"user":{"displayName":"Colton Davenport","photoUrl":"","userId":"13788767686338694290"}}},"source":["def denoise_audio(model, noisy_filepaths):\n","  num_files = len(noisy_filepaths)\n","  print(\"denoising \", num_files, \" files\")\n","  count=0\n","  num_files = len(noisy_filepaths) \n","  ten_percent = num_files // 10 + 1\n","  print (f\"denoising {num_files} noisy files\")\n","  print(\"  [.\", end=\"\")\n","  for noisy_file in noisy_filepaths:\n","    # load the noisy_audio\n","    noisyAudio, noisy_sr = read_audio(noisy_file, sample_rate=fs)\n","\n","    # extract the noisy audio's features\n","    noiseAudioFeatureExtractor = FeatureExtractor(noisyAudio, windowLength=windowLength, overlap=overlap, sample_rate=noisy_sr)\n","    noisy_stft_features = noiseAudioFeatureExtractor.get_stft_spectrogram()\n","\n","    # Paper: Besides, spectral phase was not used in the training phase.\n","    # At reconstruction, noisy spectral phase was used instead to\n","    # perform in- verse STFT and recover human speech.\n","    noisyPhase = np.angle(noisy_stft_features)\n","    noisy_stft_features = np.abs(noisy_stft_features)\n","\n","    mean = np.mean(noisy_stft_features)\n","    std = np.std(noisy_stft_features)\n","    noisy_stft_features = (noisy_stft_features - mean) / std\n","\n","    # prepare input for the generator\n","    predictors = prepare_input_features(noisy_stft_features)\n","\n","    # reshape input for the generator\n","    predictors = np.reshape(predictors, (predictors.shape[0], predictors.shape[1], 1, predictors.shape[2]))\n","    predictors = np.transpose(predictors, (3, 0, 1, 2)).astype(np.float32)\n","\n","    # run the generator\n","    STFTFullyConvolutional = model.predict(predictors)\n","    \n","    # revert features to audio\n","    denoised_audio = revert_features_to_audio(noiseAudioFeatureExtractor, STFTFullyConvolutional, noisyPhase, mean, std)\n","    \n","    # use the denoised audio filename with the same suffix as the noisy filename\n","    denoised_filename = 'denoised_' + noisy_file.split('_')[-1]\n","    denoised_write_path = os.path.join(RESULTS_DIR, denoised_filename)\n","\n","    # save the denoised audio\n","    librosa.output.write_wav(denoised_write_path, denoised_audio, int(noisy_sr))\n","\n","    # update the user every five files successfully denoised \n","    count += 1\n","    if count % ten_percent == 0:\n","      print(\".\",end=\"\")\n","  print(\"]\")\n","  print(\"All \", num_files, \" files denoised\")"],"execution_count":32,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r7NAbciM60Ub","colab_type":"text"},"source":["## Denoise Audio"]},{"cell_type":"code","metadata":{"id":"mNpHS4LuShxd","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":90},"executionInfo":{"status":"ok","timestamp":1596314870805,"user_tz":420,"elapsed":89532,"user":{"displayName":"Colton Davenport","photoUrl":"","userId":"13788767686338694290"}},"outputId":"a825901c-dce9-4152-dc67-5c158760bb2d"},"source":["# Build the model\n","model = build_model(l2_strength=0.0)\n","# Load the model's weights\n","model.load_weights(WEIGHTS_PATH)\n","# Load noisy audio filenames\n","noisy_filenames = glob.glob(os.path.join(NOISY_DIR, 'noisy_*.wav'))\n","\n","# Denoise the Audio\n","denoise_audio(model, noisy_filenames)"],"execution_count":33,"outputs":[{"output_type":"stream","text":["denoising  46  files\n","denoising 46 noisy files\n","  [..........]\n","All  46  files denoised\n"],"name":"stdout"}]}]}